{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import ResNetFeat\n",
    "import yaml\n",
    "import data\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('base_classes.json') as f:\n",
    "    base_classes=json.load(f)\n",
    "\n",
    "with open('novel_classes.json') as f:\n",
    "    novel_classes=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg='train_save_data.yaml'\n",
    "modelfile='checkpoints/ResNet10_sgm/19.tar'\n",
    "model='ResNet10'\n",
    "num_classes=10378\n",
    "batch_size=16\n",
    "maxiters=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Save features')\n",
    "    parser.add_argument('--cfg', required=True, help='yaml file containing config for data')\n",
    "    parser.add_argument('--modelfile', required=True, help='model file')\n",
    "    parser.add_argument('--model', type=str, default='ResNet10', help='model')\n",
    "    parser.add_argument('--num_classes', type=int,default=10378)\n",
    "    parser.add_argument('--batchsize', default=16, type=int)\n",
    "    parser.add_argument('--maxiters', default=1000, type=int)\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(model,data_loader):\n",
    "    \n",
    "    feature_set=[]\n",
    "    label_set=[]\n",
    "    for i, (x,y) in enumerate(data_loader):\n",
    "        # ignoring the data that belong to base class\n",
    "        index=0\n",
    "        while True:\n",
    "            if(y[index] not in novel_classes):\n",
    "                y=torch.cat([y[0:index], y[index+1:]])\n",
    "                x=torch.cat([x[0:index], x[index+1:]])\n",
    "                index-=1\n",
    "            index+=1\n",
    "\n",
    "            if(len(y)==index):\n",
    "                break\n",
    "\n",
    "        if(len(y)==0):\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        if i%10 == 0:\n",
    "            print('{:d}/{:d}'.format(i, len(data_loader)))\n",
    "        x = x.cuda()\n",
    "        x_var = Variable(x)\n",
    "        scores, feats = model(x_var)\n",
    "        feature_set.extend(feats.data.cpu().numpy())\n",
    "        label_set.extend(y)\n",
    "        \n",
    "        return (feature_set,label_set)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(features, labels,num_classes, params, batchsize=1000, maxiters=1000):\n",
    "    featdim = features[0].shape[1]\n",
    "    model = nn.Linear(featdim, num_classes)\n",
    "    model = model.cuda()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), params.lr, momentum=params.momentum, dampening=params.momentum, weight_decay=params.wd)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss_function = loss_function.cuda()\n",
    "    for i in range(maxiters):\n",
    "        (x,y) = lowshot_dataset.get_sample(batchsize)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = Variable(x.cuda())\n",
    "        y = Variable(y.cuda())\n",
    "        scores = model(x)\n",
    "\n",
    "        loss = loss_function(scores,y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i%100==0):\n",
    "            print('{:d}: {:f}'.format(i, loss.data[0]))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    with open(cfg,'r') as f:\n",
    "        data_params = yaml.load(f)\n",
    "\n",
    "    data_loader = data.get_data_loader(data_params)\n",
    "    model = get_model(model, num_classes)\n",
    "    model = model.cuda()\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    tmp = torch.load(modelfile)\n",
    "    if ('module.classifier.bias' not in model.state_dict().keys()) and ('module.classifier.bias' in tmp['state'].keys()):\n",
    "        tmp['state'].pop('module.classifier.bias')\n",
    "    \n",
    "    model.load_state_dict(tmp['state'])\n",
    "    model.eval()\n",
    "    \n",
    "#     feature_set,label_set=get_features(model,data_loader)\n",
    "#     model = training_loop(feature_set,labels, numclasses, params, batchsize, maxiters)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
